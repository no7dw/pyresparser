{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('./j_detail.csv')\n",
    "\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "import jieba\n",
    "ps = nltk.PorterStemmer()\n",
    "stopwords =[]\n",
    "with open(\"./stopwords.txt\") as file:  \n",
    "    stopwords = file.read() \n",
    "\n",
    "# stopwords =nltk.corpus.stopwords.words('english')\n",
    "all_stop_words = stopwords.split('\\n')\n",
    "\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n",
    "    tokens = re.split('\\W+', text)\n",
    "    text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return text\n",
    "\n",
    "def jiebaclearText(text, all_stop_words=''):\n",
    "    \n",
    "    mywordlist = ''\n",
    "    seg_list = jieba.cut(text, cut_all=False, HMM=True)\n",
    "    f_stop_seg_list=all_stop_words\n",
    "    for myword in seg_list:\n",
    "        if not(myword.strip() in f_stop_seg_list) and len(myword.strip())>1:\n",
    "            mywordlist = mywordlist + ' ' + myword\n",
    "    return mywordlist\n",
    "df['clean'] = df['jd'].apply(lambda x: jiebaclearText(x))\n",
    "\n",
    "\n",
    "words = ''\n",
    "for w in df['clean']:\n",
    "    words = words + ' '+ w\n",
    "words = words.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in fact need a set\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(words)\n",
    "tops=fdist.most_common(1000)\n",
    "\n",
    "\n",
    "# import collections\n",
    "# collections.Counter(words)\n",
    "\n",
    "#keep skillset only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=tops[-10:][1]\n",
    "print(r1[0])\n",
    "pattern=re.compile(\"[^A-Za-z]\")\n",
    "a=pattern.match( r1[0])\n",
    "re.findall(\"[^A-Za-z]\",r1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "\"技术\" in words.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
